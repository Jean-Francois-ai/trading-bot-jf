# trading_bot_llm_comparison_fixed.py
from openai import OpenAI
import json
import time
import requests

# Configuration OpenRouter
API_KEY = "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"
OPENROUTER_URL = "https://openrouter.ai/api/v1"

# Test de connectivitÃ©
def test_connectivity():
    print("ğŸ” Test de connectivitÃ© vers OpenRouter...")
    try:
        response = requests.get("https://openrouter.ai", timeout=5)
        print(f"âœ… ConnectivitÃ©: HTTP {response.status_code}")
        return True
    except Exception as e:
        print(f"âŒ Ã‰chec de connexion: {str(e)}")
        return False

# Test d'un modÃ¨le avec gestion des rÃ©ponses incomplÃ¨tes
def test_model(model_name, prompt, max_retries=2):
    print(f"\nğŸš€ DÃ©but du test pour: {model_name}")
    client = OpenAI(base_url=OPENROUTER_URL, api_key=API_KEY)
    
    for attempt in range(max_retries):
        try:
            print(f"  Tentative {attempt+1}/{max_retries}...")
            start_time = time.time()
            
            # Gestion spÃ©ciale pour deepseek-v3-base
            actual_model = model_name
            if "v3-base" in model_name:
                print("âš ï¸ Remplacement de deepseek-v3-base par deepseek-r1 (plus fiable)")
                actual_model = "deepseek/deepseek-r1:free"
            
            response = client.chat.completions.create(
                model=actual_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=400,  # AugmentÃ© pour Ã©viter les troncatures
                timeout=30
            )
            
            elapsed_time = time.time() - start_time
            content = response.choices[0].message.content.strip() if response.choices else ""
            
            if content:
                # VÃ©rifie si la rÃ©ponse est complÃ¨te
                is_complete = "###FIN###" in content
                content = content.replace("###FIN###", "").strip()
                
                return {
                    "success": True,
                    "model": model_name,
                    "actual_model": actual_model,
                    "time": elapsed_time,
                    "content": content,
                    "complete": is_complete,
                    "tokens": response.usage.total_tokens,
                    "finish_reason": response.choices[0].finish_reason
                }
                
        except Exception as e:
            error = str(e)
            print(f"  âŒ Erreur: {error}")
            time.sleep(1.5 * (attempt + 1))
    
    return {
        "success": False,
        "model": model_name,
        "error": error if 'error' in locals() else "RÃ©ponse vide"
    }

# --- Programme principal ---
if __name__ == "__main__":
    print("="*60)
    print("COMPARAISON COMPLÃˆTE DE MODÃˆLES LLM POUR TRADING BOT")
    print("="*60)
    
    if not test_connectivity():
        exit(1)
    
    # Prompt optimisÃ© avec instruction de fin
    test_prompt = (
        "Tu es un expert en trading algorithmique. "
        "Explique en 2 phrases MAXIMUM comment un RSI > 70 peut Ãªtre utilisÃ© "
        "dans une stratÃ©gie de trading. "
        "Important: "
        "1. Sois concis et prÃ©cis "
        "2. Termine ta rÃ©ponse par ###FIN### "
        "3. Ne donne PAS de conseil financier."
    )
    
    print(f"\nğŸ“ Prompt de test:\n{test_prompt}")
    
    # Liste des modÃ¨les Ã  comparer
    models_to_test = [
        "tngtech/deepseek-r1t2-chimera:free",   # DeepSeek Chimera
        "deepseek/deepseek-v3-base:free",        # DeepSeek V3 Base (sera remplacÃ©)
        "mistralai/mistral-small-3.1-24b-instruct:free", # Mistral 24B
        "gryphe/mythomax-l2-13b:free"            # MythoMax (alternative)
    ]
    
    # Tester tous les modÃ¨les
    results = []
    for i, model in enumerate(models_to_test):
        print(f"\n{'='*40} TEST {i+1}/{len(models_to_test)} {'='*40}")
        result = test_model(model, test_prompt)
        results.append(result)
        time.sleep(1)  # Pause entre les modÃ¨les
    
    # Afficher les rÃ©sultats comparatifs
    print("\n\nğŸ“Š RÃ‰SULTATS COMPARATIFS")
    print("="*80)
    
    for res in results:
        if res['success']:
            status = "âœ… COMPLET" if res['complete'] else "âš ï¸ INCOMPLET (marqueur ###FIN### manquant)"
            print(f"\nğŸ”¹ ModÃ¨le demandÃ©: {res['model']}")
            if res['model'] != res.get('actual_model', ''):
                print(f"ğŸ”¹ ModÃ¨le utilisÃ©: {res['actual_model']}")
            print(f"â± Temps: {res['time']:.2f}s | Tokens: {res['tokens']} | {status}")
            print(f"ğŸ“ RÃ©ponse:\n{res['content']}")
            print("-"*80)
        else:
            print(f"\nâŒ Ã‰chec avec {res['model']}: {res.get('error', 'Raison inconnue')}")
    
    # Statistiques
    success_count = sum(1 for res in results if res['success'])
    complete_count = sum(1 for res in results if res.get('complete', False))
    
    print(f"\nğŸ“ˆ STATISTIQUES FINALES")
    print(f"âœ… {success_count}/{len(models_to_test)} modÃ¨les ont rÃ©pondu")
    print(f"ğŸŸ¢ {complete_count} rÃ©ponses complÃ¨tes (avec ###FIN###)")
    print(f"âš ï¸ {success_count - complete_count} rÃ©ponses incomplÃ¨tes")
    
    # Trouver le modÃ¨le le plus rapide parmi ceux qui ont rÃ©ussi
    successful_models = [res for res in results if res['success']]
    if successful_models:
        fastest = min(successful_models, key=lambda x: x['time'])
        print(f"âš¡ ModÃ¨le le plus rapide: {fastest.get('actual_model', fastest['model'])} ({fastest['time']:.2f}s)")

# ExÃ©cutez ce code pour obtenir une comparaison complÃ¨te!
